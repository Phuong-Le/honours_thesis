\newpage
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{section}{0}
\setcounter{equation}{0}
\renewcommand\thesection{A.\arabic{section}} 
\renewcommand\thefigure{A\arabic{figure}} 
\renewcommand\thetable{A\arabic{table}} 
\renewcommand\theequation{A\arabic{table}} 
\fancyfoot[RO,LE]{APPENDICES}

\chapter*{Appendices}
\addcontentsline{toc}{chapter}{Appendices}

\section{Cancers of interest and putative original cells}
\input{tables/apdx_original_cells}

\section{PCAWG mutation summary}
\vspace{1cm}
\input{tables/apdx_mutation_summary}
\vspace{2cm}
\input{figures/apdx_mutation_summary}

% \newpage
% \section{How cancer original cell types are related by DHS}
% \input{tables/apdx_encode_pca}

\newpage
\section{Supplementary computations for smoothing GLE}
\subsection{Gaussian kernel density estimation}

Kernel functions estimate the density of data at a particular location by measuring the distance from that location to all observed data points. To estimate mutation density across the genome, I used the Gaussian kernel, as follows:

\begin{equation}
    K(u) = \frac{1}{2\pi} e^{\frac{-u^2}{2}}
    \label{eq:gaussian}
\end{equation}

where $K$ is the kernel function, $u$ is a variable - \textit{i.e.} $\frac{x-X{i}}{h}$ in equation \ref{eq:density}

\subsection{Bandwidth choice}
The bandwidth $h$ determines the level of smoothing, or the distance at which the an observed mutation can meaningfully influence the density $\hat{f}$ at location $x$, I used the Scott's method, as follows:

\begin{equation}
    h = 1.059 A n^{-1/5}
    \label{eq:bandwidth}
\end{equation}

where $h$ is the bandwidth, $A$ is a measure of spread of the observed mutation locations (\textit{i.e.} the smaller of (1) the standard deviation and the (2) inter-quartile range) and $n$ is the total number of mutations.


\newpage
\section{Supplementary computations for SCE}

\subsection{Deviance residuals for GLM of contingency table}
\begin{equation}
    r_i = sign(y_i - \hat{\mu}_i) \times \sqrt{2y_i\ln{\frac{y_i}{\hat{\mu}_i}} - (y_i - \hat{\mu}_i)}
    \label{eq:dev_res}
\end{equation}
where $r_i$ is the deviance residual for entry $i$, $y_i$ is the actual observed count of that entry, $\hat{\mu}_i$ is the estimated expected count for that entry under the saturated model. The $sign$ function determines the sign of the residual $r_i$, \textit{i.e.} whether it is positive ($>$0) or negative ($<$0). In particular, $r_i>0$ if the observed is greater than the expected and vice versa. 

\subsection{Fisher's method}\label{apdx:fisher}
To aggregate $k$ p-values, we could calculate the Fisher statistic $X$

\begin{equation}
    X = 2 \sum_{i=1}^k \ln{p_i}
    \label{eq:fisher}
\end{equation}
where $X$ is the Fisher statistic, $p_i$ is the member p-value. If converted from the log scale as seen in equation \ref{eq:fisher} to the normal scale, we could see that this is essentially the square of the product of the p-values. $X$ follows the $\chi^2_{2k}$ distribution, so we can easily obtain an aggregated p-value by contrasting $X$ against $\chi^2_{2k}$.

\newpage
\section{Supplementary computations for training classifier}

\subsection{Accuracy measures}

\subsubsection{Sensitivity}
\begin{equation}
    sensitivity_A = \frac{A_{pred(T)}}{A_{true}} 
    \label{eq:sensitivity}
\end{equation}
where $sensitivity_A$ is the sensitivity for identifying A; $A_{pred}$ is the number of A's correctly predicted; $A_{true}$ is the number of true A's

\subsubsection{Specificity}
\begin{equation}
    specificity_A = \frac{A_{pred(T)}}{A_{pred}}
    \label{eq:specificity}
\end{equation}
where $specificity_A$ is the specificity for an observation to be A if it is predicted to be A; $A_{pred(T)}$ is the number of correctly predicted A; $A_{pred}$ is the number of observations predicted to be A.

\subsection{Kullback-Leibler divergence}
The Kullback Leibler divergence of M from P is calculated as follows:

\begin{equation}
    d_{KL}(P|M) = \sum_{x \in X} p(x) \ln{\frac{p(x)}{m(x)}}
    \label{eq:kl}
\end{equation}
where $X$ is the probability space on which $P$ and $M$ are defined; $p$ and $m$ represent the probability of $P$ and $Q$ at $x$.

\subsection{Converting kernel to distance matrix}

\begin{equation}
    d(x,y) = \sqrt{k(x,x) + k(y,y) - 2k(x,y)} 
    \label{eq:k2d_ori}
\end{equation}
where $d(x,y)$ is the entry for $x$ and $y$ to the joint distance matrix; $k(x,y)$ is the entry to the kernel matrix $J$. Because using the Laplacian kernel forces the diagonals, or $k(x,x)$ and $k(y,y)$, to be 1, the formula could be simplified as in \ref{eq:k2d}.

\newpage
\section{Mutation density with respect to chromatin status}

\input{figures/apdx_mutation_density}

\newpage
\section{Contingency tables for G-test of independence}\label{apdx:g-test}

\input{tables/apdx_g-test}

\newpage
\section{Mislabelled DHS data of chromatin status}
\input{figures/apdx_mixed_or_by_row}

\newpage
\section{Base substitutions}

\input{figures/apdx_spectra}

\newpage
\section{Supplementary confusion matrices}



