\chapter{Classification}\label{ml}

Chapter \ref{gle} demonstrates that the \gls{gle} contributes to characterising the cancer mutagenesis patterns. Chapter \ref{sce} shows evidence that there is an informational advantage in exploiting the bases outside 3-mers surrounding the changed bases. Nonetheless, to manipulate the information from \gls{gle} and \gls{sce}, they have to be represented correctly. A sensible instinct is that a more suitable representation of a feature gives higher accuracy than a less suitable representation. This chapter acts as both an application and a validation for chapters \ref{gle} and \ref{sce}. In particular, section \ref{ml:gle} shows that the smooth representation of \ref{gle} is a better representation than binning the genome. Section \ref{ml:sce} shows that \textcolor{blue}{blah blah}. Additionally, section \ref{ml:both} demonstrates how to combine two factors with different units, like GLE and SCE, in a joint model in an attempt to further improve accuracy and to weigh the importance of each in the presence of the other.

While each of the next sections works on different inputs, all sections follow the same procedure for training the classifier (\ref{methods:ml_workflow}). Briefly, a small proportion of data is set aside as a test set, which is then used to evaluate model performance.

\section{Classifier based on GLE}\label{ml:gle}
This section seeks to identify an optimal approach to extract the most information from GLE for cancer prediction. Specifically, I trialled the bin \textit{v.s.} smoothing representations together with the Euclidean \textit{v.s.} Wasserstein distances. I iterated the training procedures 10 times to estimate the range of the $F1$ score obtained from these approaches. $F1$ was chosen as the accuracy measure as it takes into consideration both sensitivity and specificity. This is summarised in Figure \ref{fig:f1_gle}. For Euclidean, the smoothing was much better than the bin approach. This was also very clear when inspecting the confusion matrices (Figure \ref{fig:ml_gle}), where many more observations lied on the diagonals. This is consistent with the bootstrap results from section \ref{gle:bootstrap}. For Wasserstein, the difference between the smoothing and bin approach was not obvious (Figure \ref{fig:f1_gle} and \ref{}). Overall, the results suggest that the smoothing representation has a higher predictive power and that the Wasserstein distance has some interesting properties, discussed in \ref{}.

\input{figures/ml_gle}

\newpage
\section{Classifier based on SCE}\label{ml:sce}

\subsection{3-mer is the most accurate sequence context size}
To train a classifier based on SCE, I experimented with incorporating the flanking bases to the mutation composition, including the context sizes of 1-mer, 3-mer and 5-mer, where 1-mer is the base substitution without any context. In parallel, I trialled imposing asymmetry (no symmetry), semi-symmetry (reverse complementary strand symmetry) and full-symmetry (reverse complementary strand symmetry and flanking bases restricted to be A and C). Figure \ref{fig:f1_sce} shows the $F1$ for these approaches. In general, SCE was more accurate then GLE. However, the fully symmetric representation severely dropped in accuracy when flanking bases were involved, which suggests that it introduced noise to the data. Asymmetry and semi-symmetry performed roughly similarly, with 3-mer being the most accurate representation, reinforcing the evidence of strand symmetry previously seen in \ref{sce}. Assuming asymmetry and semi-symmetry is equivalent, $F1$ being lower for 5-mer than 3-mer but higher for semi-symmetry than asymmetry is very curious. It suggests the potential impact of splitting up mutation counts into too many elements in 5-mer (Table \ref{tab:sce_symmetric}).

\input{figures/ml_sce}

\subsection{Dissecting 5-mer into submotifs can potentially improve accuracy}
Suspecting that the poor performance of 5-mer was due to the long vector that represented it, I tried splitting up 5-mers into smaller vectors (submotifs) then using the linear combination of them as input into the classifier (Table \ref{tab:submotif}). To demonstrate, for 2-submotifs, I split the long 5-mer vector into 4 shorter vectors that only involved 2 positions, including the base substitution. Figure \ref{fig:f1_sce_submotif} shows that the splitting did improve $F1$ compared to the original whole 5-mer vector, particularly the 3-submotif representation. This is very promising, but at this point it is uncertain whether it was the information from outer flanking positions or the 3-mer incorporated in the 3-motifs that drove this accuracy improvement.

\input{figures/ml_sce_submotif}

\section{Classifier based on the combination of GLE and SCE}\label{ml:both}

\input{figures/ml_combined}