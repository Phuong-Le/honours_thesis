\setglossarypreamble{This list contains the most relevant versions of definitions to this research project}
\makeglossaries
\newglossaryentry{gle}
{
        name=GLE,
        description={(Genomic Location Effect) \\ the influence of a nucleotide's location on the tendency for it to mutate}
}

\newglossaryentry{nucleotide}
{
        name=nucleotide,
        description={same as base; a component of the DNA sequence, \textit{e.g.} A for adenine}
}

\newglossaryentry{base}
{
        name=base,
        description={same as nucleotide; a component of the DNA sequence, \textit{e.g.} A for adenine}
}

\newglossaryentry{sce}
{
        name=SCE,
        description={(Sequence Context Effect) \\ the influence of the flanking bases on a nucleotide's location on the tendency for it to mutate}
}

\newglossaryentry{or}
{
        name=$OR$,
        description={(Odds ratio) \\ a statistic that measures the degree bias for two characteristics to occur. In my case, I measured how mutations favour closed over open chromatin regions}
}

\newglossaryentry{glm}
{
        name=GLM,
        description={(Generalised Linear Model) \\ A class of model that allows the errors (residuals) to follow a different distribution than the normal distribution. The ordinary linear model is one example of this class}
}

\newglossaryentry{pcawg}
{
        name=PCAWG,
        description={(Pan-cancer analysis of whole genome) \\ an international project that sequenced samples of different cancer types. Part of the project also identified somatic mutations and passenger mutations.}
}

\newglossaryentry{icgc}
{
        name=ICGC,
        description={(International Cancer Genome Consortium) \\ an organisation that contributes to the PCAWG project. Access to the mutation file (\textit{i.e.} non US data) from ICGC is not restricted}
}

\newglossaryentry{tcga}
{
        name=TCGA,
        description={(The Cancer Genome Atlas) \\ another contributor to the PCAWG project. Access to the mutation file (\textit{i.e.} US data) from TCGA is restricted}
}

\newglossaryentry{encode}
{
        name=ENCODE,
        description={(Encyclopedia of DNA Elements) \\ a project that studies various genetics and epigenetics data}
}

\newglossaryentry{dhs}
{
        name=DHS,
        description={(DNase Hypersensitivity) \\ a measure of chromatin status - how accessible a genomic region is. Applying the threshold to dnase accessibility helps identify open vs closed genomic regions. This project used data from ENCODE for DHS, where open and closed chromatin regions of the genome have been identified}
}

\newglossaryentry{chromatin}
{
        name=chromatin,
        description={a complex of DNA with histone protein. A genomic region with a very dense complex is not easily accessible to factors like transcription and DNA repair. Such a region is called closed chromatin region, vice versa for open chromatin regions}
}

\newglossaryentry{null}
{
        name=$H_o$,
        description={(Null hypothesis) \\ The null hypothesis assumes a certain characteristic is the same between 2 or more different classes}
}

\newglossaryentry{alternative}
{
        name=$H_a$,
        description={(Alternative hypothesis) \\ The alternative hypothesis assumes a certain characteristic is different between 2 or more different given classes.}
}

\newglossaryentry{f1}
{
        name=$F_1$,
        description={(F1-Score) \\ A measure of accuracy that takes into account both the sensitivity and specificity of the classifier}
}

\newglossaryentry{sensitivity}
{
        name=sensitivity,
        description={a measure of accuracy for how well a classifier can identify certain classes. The sensitivity for a class is the ratio between the number of observations correctly identified and the total number of observations available for that class}
}

\newglossaryentry{specificity}
{
        name=specificity,
        description={a measure of accuracy for how well a classifier can exclude the possibility of other classes when identifying certain classes. The specificity for a class is the ratio between the number of observations correctly identified as that class and the total number of observations identified as that class}
}

\newglossaryentry{confusion matrix}
{
        name=confusion matrix,
        plural=confusion matrices,
        description={a way to report accuracy of a classifier. In this case the row labels are the true classes and the column labels are the predicted classes. The more data points on the diagonals, the higher the accuracy}
}

\newglossaryentry{ml}
{
        name=ML,
        description={(Machine Learning) \\ the process of building a model to learn the patterns of data}
}

\newglossaryentry{model}
{
        name=model,
        description={all the statistical assumptions about the properties and patterns of data}
}

\newglossaryentry{classifier}
{
        name=classifier,
        description={a model that predict categorical data. In other words, the responses of the model are categorical, not numeric}
}

\newglossaryentry{kde}
{
        name=KDE,
        description={(Kernel Density Estimation) \\ the use of a kernel function to estimate the density of a particular data point}
}

\newglossaryentry{kernel function}
{
        name=kernel function,
        description={any function that belongs to a class of continuous and symmetric functions commonly used in density estimation and computation of ``similarity'' between 2 vectors. When applied on every pair of vectors to obtain a kernel matrix, this matrix is positive semi-definite.}
}

\newglossaryentry{positive semi-definite}
{
        name = positive definite,
        description={a matrix $M$ is positive definite if for every element vector $z$, $zMz^\intercal > 0$. In other words, the eigenvalues of $M$ are positive. Additionally, $M$ is positive semi-definite if its eigenvalues are non-negative}
}

\newglossaryentry{density}
{
        name=density,
        description={the density measures the likelihood of finding data $f(x)$ at a particular location $x$. The density throughout the domain of the data is restricted to be 1}
}

\newglossaryentry{intensity}
{
        name=intensity,
        description={the intensity of a data point is the product of its density and the sample size of the data. Accordingly, while ``inheriting'' some characteristics of the density, the intensity throughout the domain of the data is not restricted to be 1}
}

\newglossaryentry{knn}
{
        name=KNN,
        description={($k$-nearest neighbours) \\ a machine learning algorithm that predicts the labels of a data point based on the information from the closest data points to it. The number of closest data points $k$ is a hyper-parameter to be pre-defined. The choice of the distance measures used to identify the closest data points usually depend on the nature of the data}
}

\newglossaryentry{svm}
{
        name=SVM,
        description={(Support Vector Machines) \\ a machine learning algorithm that predicts the labels of a data point based on finding the boundaries between different classes based on the data points near the border of each class. Those data points are also called the support vectors}
}

\newglossaryentry{pval}
{
        name=$p$-value,
        description={the probability for an event to occur assuming the null hypothesis $H_o$ is true. The smaller the $p$, the more evidence we have to reject the null hypothesis}
}

\newglossaryentry{jackknife}
{
        name=jackknife,
        description={a statistical re-sampling technique which is generally used to estimate the variance in a particular statistic of a data sample. This technique re-computes the statistic of interest upon removing one data point from the given sample. The departure of the re-computed statistic from the original statistic indicates how extreme and influential the removed data point is. Repeating the procedure for every data point outputs a collection of re-computed statistics, which represents the estimated range for that statistic}
}

\newglossaryentry{bootstrap}
{
        name=bootstrap,
        description={a statistical re-sampling technique that reconstructs the original data set by randomly drawing data points from that original data set. The idea is that the reconstructed data set should inherit the properties of the original set, and the departure arises mainly from random noise. Similar to the jackknife, the bootstrap can be used to estimate data variance. However, in this project, the bootstrap was used for hypothesis testing}
}

\newglossaryentry{kmer}
{
        name=$k$-mer,
        description={a $k$ base long DNA sequence unit. For example, ACC and GAT are 3-mers while GCTAC is a 5-mer}
}

\newglossaryentry{re}
{
        name=$RE$,
        description={Relative Entropy \\
        a statistic that measures the amount of information available. Generally, information essentially implies variation. Here $RE$ is derived from a GLM.}
}

\newglossaryentry{sommut}
{
        name=somatic mutation,
        description={a mutation that arises in a somatic (non-germline) cell. This mutation can be passed on to the daughter cells of that somatic cell, but not to the offspring of the person that has the mutation}
}

\newglossaryentry{germline_mut}
{
        name=germline mutation,
        description={a mutation that arises in a germline cell. This mutation might be passed on to the offspring of the person that has the mutation.}
}

\newglossaryentry{point_mut}
{
        name=point mutation,
        description={a mutation where there is one single base change to another base}
}

\newglossaryentry{carcinogenesis}
{
        name=carcinogenesis,
        description={the process of cancer development}
}

\newglossaryentry{mutagenesis}
{
        name=mutagenesis,
        description={the process of generating and developing mutations}
}

\newglossaryentry{mutagen}
{
        name=mutagen,
        description={an agent that causes mutations, \textit{e.g.} UV radiation and chemicals}
}


\newglossaryentry{mut_profile}
{
        name=mutation profile,
        description={the record of all mutations present in a cancer sample}
}